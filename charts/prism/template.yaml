---
# Source: prism/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: prism-dev-redis
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: prism/charts/nats/templates/pod-disruption-budget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: nats
---
# Source: prism/charts/redis/templates/master/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: prism-dev-redis-master
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: master
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
---
# Source: prism/charts/redis/templates/replicas/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: prism-dev-redis-replicas
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: replica
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: replica
---
# Source: prism/charts/jaeger/templates/collector-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: magistrala-jaeger-collector
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
automountServiceAccountToken: false
---
# Source: prism/charts/jaeger/templates/query-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: magistrala-jaeger-query
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
automountServiceAccountToken: false
---
# Source: prism/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: prism-dev-redis-master
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
---
# Source: prism/charts/redis/templates/replicas/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: prism-dev-redis-replica
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
---
# Source: prism/charts/vault/templates/injector-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prism-dev-vault-agent-injector
  namespace: prism-dev
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
---
# Source: prism/charts/vault/templates/server-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prism-dev-vault
  namespace: prism-dev
  labels:
    helm.sh/chart: vault-0.28.1
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
---
# Source: prism/charts/nats/templates/nats-box/contexts-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: nats-box
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats-box-contexts
stringData:
  default.json: |
    {
      "url": "nats://prism-dev-nats"
    }
type: Opaque
---
# Source: prism/charts/postgresqlauth/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: prism-dev-postgresqlauth
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlauth
    helm.sh/chart: postgresqlauth-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
type: Opaque
data:
  postgres-password: "bWFnaXN0cmFsYQ=="
  password: "bWFnaXN0cmFsYQ=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: prism/charts/postgresqlspicedb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: prism-dev-postgresqlspicedb
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlspicedb
    helm.sh/chart: postgresqlspicedb-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
type: Opaque
data:
  postgres-password: "bWFnaXN0cmFsYQ=="
  password: "bWFnaXN0cmFsYQ=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: prism/charts/postgresqlusers/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: prism-dev-postgresqlusers
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlusers
    helm.sh/chart: postgresqlusers-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
type: Opaque
data:
  postgres-password: "bWFnaXN0cmFsYQ=="
  password: "bWFnaXN0cmFsYQ=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: prism/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: prism-dev-redis
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
type: Opaque
data:
  redis-password: "T3R1WjBJTHhLUA=="
---
# Source: prism/charts/nats/templates/config-map.yaml
apiVersion: v1
data:
  nats.conf: |
    {
      "http_port": 8222,
      "jetstream": {
        "max_file_store": 10Gi,
        "max_memory_store": 2Gi,
        "store_dir": "/data"
      },
      "lame_duck_duration": "30s",
      "lame_duck_grace_period": "10s",
      "pid_file": "/var/run/nats/nats.pid",
      "port": 4222,
      "server_name": $SERVER_NAME
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats-config
---
# Source: prism/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-redis-configuration
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: prism/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-redis-health
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: prism/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-redis-scripts
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")
    HEADLESS_SERVICE="prism-dev-redis-headless.prism-dev.svc.cluster.local"

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_MASTER_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: prism/charts/vault/templates/server-config-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-vault-config
  namespace: prism-dev
  labels:
    helm.sh/chart: vault-0.28.1
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
data:
  extraconfig-from-values.hcl: |-
    
    disable_mlock = true
    ui = true
    
    listener "tcp" {
      tls_disable = 1
      address = "[::]:8200"
      cluster_address = "[::]:8201"
      # Enable unauthenticated metrics access (necessary for Prometheus Operator)
      #telemetry {
      #  unauthenticated_metrics_access = "true"
      #}
    }
    storage "file" {
      path = "/vault/data"
    }
    
    # Example configuration for using auto-unseal, using Google Cloud KMS. The
    # GKMS keys must already exist, and the cluster must have a service account
    # that is authorized to access GCP KMS.
    #seal "gcpckms" {
    #   project     = "vault-helm-dev"
    #   region      = "global"
    #   key_ring    = "vault-helm-unseal-kr"
    #   crypto_key  = "vault-helm-unseal-key"
    #}
    
    # Example configuration for enabling Prometheus metrics in your config.
    #telemetry {
    #  prometheus_retention_time = "30s"
    #  disable_hostname = true
    #}
---
# Source: prism/templates/auth-deployment.yaml
# Copyright (c) Ultraviolet
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-spicedb-schema-zed
data:
  schema.zed: |-
    definition user {}
    
    definition thing {
    	relation administrator: user
    	relation group: group
    	relation domain: domain
    
    	permission admin = administrator + group->admin + domain->admin
    	permission delete = admin
    	permission edit = admin + group->edit + domain->edit
    	permission view = edit + group->view  + domain->view
    	permission share = edit
    	permission publish = group
    	permission subscribe = group
    
    	// These permission are made for only list purpose. It helps to list users have only particular permission excluding other higher and lower permission.
    	permission admin_only = admin
    	permission edit_only = edit - admin
    	permission view_only = view
    
    	// These permission are made for only list purpose. It helps to list users from external, users who are not in group but have permission on the group through parent group
    	permission ext_admin = admin - administrator // For list of external admin , not having direct relation with group, but have indirect relation from parent group
    }
    
    definition group {
    	relation administrator: user
    	relation editor: user
    	relation contributor: user
    	relation member: user
    	relation guest: user
    
    	relation parent_group: group
    	relation domain: domain
    
    	permission admin =  administrator + parent_group->admin + domain->admin
    	permission delete = admin
    	permission edit = admin + editor + parent_group->edit  + domain->edit
    	permission share = edit
    	permission view = contributor + edit + parent_group->view + domain->view + guest
    	permission membership = view + member
    	permission create = membership - guest
    
    	// These permissions are made for listing purposes. They enable listing users who have only particular permission excluding higher-level permissions users.
    	permission admin_only = admin
    	permission edit_only = edit - admin
    	permission view_only = view
    	permission membership_only = membership - view
    
    	// These permission are made for only list purpose. They enable listing users who have only particular permission from parent group excluding higher-level permissions.
    	permission ext_admin = admin - administrator  // For list of external admin , not having direct relation with group, but have indirect relation from parent group
    	permission ext_edit = edit - editor  // For list of external edit , not having direct relation with group, but have indirect relation from parent group
    	permission ext_view = view - contributor  // For list of external view , not having direct relation with group, but have indirect relation from parent group
    }
    
    definition domain {
    	relation administrator: user // combination domain + user id
    	relation editor: user
    	relation contributor: user
    	relation member: user
    	relation guest: user
    
    	relation platform: platform
    
    	permission admin = administrator + platform->admin
    	permission edit =  admin + editor
    	permission share = edit
    	permission view = edit + contributor + guest
    	permission membership = view + member
    	permission create = membership - guest
    }
    
    definition platform {
      relation administrator: user
      relation member: user
    
      permission admin = administrator
      permission membership = administrator + member
    }
---
# Source: prism/templates/spicedb-envoy-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-spicedb-envoy-config
data:
  envoy.yaml: |-
    static_resources:
      listeners:
        - address:
            socket_address:
              address: 0.0.0.0
              port_value: 50051
          filter_chains:
            - filters:
                - name: envoy.filters.network.http_connection_manager
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                    codec_type: AUTO
                    stat_prefix: prism-dev-spicedb-envoy
                    route_config:
                      name: prism-dev-spicedb_route
                      virtual_hosts:
                        - name: prism-dev-spicedb
                          domains: ["*"]
                          routes:
                            - match: { prefix: "/" }
                              route:
                                cluster: spicedb_grpc_cluster
                                timeout: 0s
                    http_filters:
                      - name: envoy.filters.http.grpc_web
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.grpc_web.v3.GrpcWeb
                      - name: envoy.filters.http.router
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
      clusters:
        - name: spicedb_grpc_cluster
          connect_timeout: 0.25s
          type: STRICT_DNS
          lb_policy: ROUND_ROBIN
          http2_protocol_options: {}
          load_assignment:
            cluster_name: spicedb_grpc_cluster
            endpoints:
              - lb_endpoints:
                - endpoint:
                    address:
                      socket_address:
                        address: prism-dev-spicedb
                        port_value: 50051
    admin:
      access_log_path: "/dev/null"
      address:
        socket_address:
          address: 0.0.0.0
          port_value: 9901
---
# Source: prism/templates/users-deployment.yaml
# Copyright (c) Ultraviolet
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: prism-dev-users-config
data:
  email.tmpl: |
    To: {{range $index, $v := .To}}{{if $index}},{{end}}{{$v}}{{end}}
    From: {{.From}}
    Subject: {{.Subject}}
    {{.Header}}
    You have initiated password reset.
    Follow the link below to reset password.
    {{.Content}}
    {{.Footer}}
---
# Source: prism/charts/vault/templates/injector-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prism-dev-vault-agent-injector-clusterrole
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: ["admissionregistration.k8s.io"]
  resources: ["mutatingwebhookconfigurations"]
  verbs:
    - "get"
    - "list"
    - "watch"
    - "patch"
---
# Source: prism/charts/vault/templates/injector-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prism-dev-vault-agent-injector-binding
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prism-dev-vault-agent-injector-clusterrole
subjects:
- kind: ServiceAccount
  name: prism-dev-vault-agent-injector
  namespace: prism-dev
---
# Source: prism/charts/vault/templates/server-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prism-dev-vault-server-binding
  labels:
    helm.sh/chart: vault-0.28.1
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: prism-dev-vault
  namespace: prism-dev
---
# Source: prism/charts/jaeger/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.15.3
    release: prism-dev
    heritage: Helm
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: intra
    port: 7000
    targetPort: 7000
  - name: tls
    port: 7001
    targetPort: 7001
  - name: jmx
    port: 7199
    targetPort: 7199
  - name: cql
    port: 9042
    targetPort: 9042
  - name: thrift
    port: 9160
    targetPort: 9160
  selector:
    app: cassandra
    release: prism-dev
---
# Source: prism/charts/jaeger/templates/collector-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: magistrala-jaeger-collector
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
spec:
  ports:
  - name: grpc
    port: 14250
    protocol: TCP
    targetPort: grpc
    appProtocol: grpc
  - name: http
    port: 14268
    protocol: TCP
    targetPort: http
    appProtocol: http
  - name: otlp-grpc
    port: 4317
    protocol: TCP
    targetPort: otlp-grpc
  - name: otlp-http
    port: 4318
    protocol: TCP
    targetPort: otlp-http
  - name: admin
    port: 14269
    targetPort: admin
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: collector
  type: ClusterIP
---
# Source: prism/charts/jaeger/templates/query-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: magistrala-jaeger-query
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  ports:
  - name: query
    port: 80
    protocol: TCP
    targetPort: query
  - name: grpc
    port: 16685
    protocol: TCP
    targetPort: grpc
  - name: admin
    port: 16687
    protocol: TCP
    targetPort: admin
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: query
  type: ClusterIP
---
# Source: prism/charts/nats/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats-headless
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/name: nats
---
# Source: prism/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats
spec:
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/name: nats
---
# Source: prism/charts/postgresqlauth/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlauth-hl
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlauth
    helm.sh/chart: postgresqlauth-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresqlauth
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/postgresqlauth/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlauth
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlauth
    helm.sh/chart: postgresqlauth-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresqlauth
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/postgresqlspicedb/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlspicedb-hl
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlspicedb
    helm.sh/chart: postgresqlspicedb-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresqlspicedb
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/postgresqlspicedb/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlspicedb
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlspicedb
    helm.sh/chart: postgresqlspicedb-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresqlspicedb
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/postgresqlusers/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlusers-hl
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlusers
    helm.sh/chart: postgresqlusers-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresqlusers
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/postgresqlusers/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-postgresqlusers
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlusers
    helm.sh/chart: postgresqlusers-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresqlusers
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/component: primary
---
# Source: prism/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-redis-headless
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/name: redis
---
# Source: prism/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-redis-master
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: prism/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-redis-replicas
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: replica
---
# Source: prism/charts/vault/templates/injector-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-vault-agent-injector-svc
  namespace: prism-dev
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
  
spec:
  ports:
  - name: https
    port: 443
    targetPort: 8080
  selector:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    component: webhook
---
# Source: prism/charts/vault/templates/server-headless-service.yaml
# Service for Vault cluster
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-vault-internal
  namespace: prism-dev
  labels:
    helm.sh/chart: vault-0.28.1
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    vault-internal: "true"
  annotations:

spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: "http"
      port: 8200
      targetPort: 8200
    - name: https-internal
      port: 8201
      targetPort: 8201
  selector:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    component: server
---
# Source: prism/charts/vault/templates/server-service.yaml
# Service for Vault cluster
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-vault
  namespace: prism-dev
  labels:
    helm.sh/chart: vault-0.28.1
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
  annotations:

spec:
  # We want the servers to become available even if they're not ready
  # since this DNS is also used for join operations.
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 8200
      targetPort: 8200
    - name: https-internal
      port: 8201
      targetPort: 8201
  selector:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    component: server
---
# Source: prism/templates/auth-service.yaml
# Copyright (c) Ultraviolet
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: prism-dev-auth
spec:
  selector:
    app: prism-dev
    component: auth
  ports:
    - protocol: TCP
      port: 8189
      name: prism-dev-auth-http-8189
    - protocol: TCP
      port: 8181
      name: prism-dev-auth-grpc-8181
---
# Source: prism/templates/auth-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-auth-headless
spec:
  selector:
    app: prism-dev
    component: auth
  ports:
    - protocol: TCP
      port: 8189
      name: prism-dev-auth-http-8189
    - protocol: TCP
      port: 8181
      name: prism-dev-auth-grpc-8181
  clusterIP: None
---
# Source: prism/templates/spicedb-envoy-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-spicedb-envoy
spec:
  selector:
    app: prism-dev
    component: spicedb-envoy
  ports:
    - port: 50051
      protocol: TCP
      name: prism-dev-spicedb-envoy-grpc-50051
---
# Source: prism/templates/spicedb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-spicedb
spec:
  selector:
    app: prism-dev
    component: spicedb
  type: ClusterIP
  ports:
    - port: 50051
      protocol: TCP
      name: prism-dev-spicedb-grpc-50051
    - port: 9090
      protocol: TCP
      name: prism-dev-spicedb-metrics-9090
---
# Source: prism/templates/spicedb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prism-dev-spicedb-headless
spec:
  selector:
    app: prism-dev
    component: spicedb
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 50051
      protocol: TCP
      name: prism-dev-spicedb-grpc-50051
---
# Source: prism/templates/users-service.yaml
# Copyright (c) Ultraviolet
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: prism-dev-users
spec:
  selector:
    app: prism-dev
    component: users
  ports:
    - protocol: TCP
      port: 9002
      name: prism-dev-users-http
---
# Source: prism/charts/jaeger/templates/collector-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: magistrala-jaeger-collector
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/component: collector
  template:
    metadata:
      annotations:
        checksum/config-env: 75a11da44c802486bc6f65640aa48a730f0f684c5c07a42ba3cd1735eb3fb070
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/component: collector
    spec:
      securityContext:
        {}
      serviceAccountName: magistrala-jaeger-collector
      
      containers:
      - name: magistrala-jaeger-collector
        securityContext:
          {}
        image: jaegertracing/jaeger-collector:1.53.0
        imagePullPolicy: IfNotPresent
        args:
          
          
          
        env:
          - name: COLLECTOR_OTLP_ENABLED
            value: "true"
          - name: SPAN_STORAGE_TYPE
            value: memory
          
        ports:
        - containerPort: 14250
          name: grpc
          protocol: TCP
        - containerPort: 14268
          name: http
          protocol: TCP
        - containerPort: 14269
          name: admin
          protocol: TCP
        - containerPort: 4317
          name: otlp-grpc
          protocol: TCP
        - containerPort: 4318
          name: otlp-http
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /
            port: admin
        livenessProbe:
          httpGet:
            path: /
            port: admin
        resources:
          {}
        volumeMounts:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
---
# Source: prism/charts/jaeger/templates/query-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: magistrala-jaeger-query
  labels:
    helm.sh/chart: jaeger-3.1.1
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/component: query
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/component: query
    spec:
      securityContext:
        {}
      serviceAccountName: magistrala-jaeger-query
       
      containers:
      - name: magistrala-jaeger-query
        securityContext:
          {}
        image: jaegertracing/jaeger-query:1.53.0
        imagePullPolicy: IfNotPresent
        args:
          
          
          
        env:
          - name: SPAN_STORAGE_TYPE
            value: memory
          
          - name: QUERY_BASE_PATH
            value: "/"
          - name: JAEGER_AGENT_PORT
            value: "6831"
        ports:
        - name: query
          containerPort: 16686
          protocol: TCP
        - name: grpc
          containerPort: 16685
          protocol: TCP
        - name: admin
          containerPort: 16687
          protocol: TCP
        resources:
          {}
        volumeMounts:
        livenessProbe:
          httpGet:
            path: /
            port: admin
        readinessProbe:
          httpGet:
            path: /
            port: admin
      - name: magistrala-jaeger-agent-sidecar
        securityContext:
          {}
        image: jaegertracing/jaeger-agent:1.53.0
        imagePullPolicy: IfNotPresent
        args:
        env:
        - name: REPORTER_GRPC_HOST_PORT
          value: magistrala-jaeger-collector:14250
        ports:
        - name: admin
          containerPort: 14271
          protocol: TCP
        resources:
          null
        volumeMounts:
        livenessProbe:
          httpGet:
            path: /
            port: admin
        readinessProbe:
          httpGet:
            path: /
            port: admin
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
---
# Source: prism/charts/nats/templates/nats-box/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: nats-box
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats-box
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats-box
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: nats
  template:
    metadata:
      labels:
        app.kubernetes.io/component: nats-box
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nats
        app.kubernetes.io/version: 2.10.17
        helm.sh/chart: nats-1.2.1
    spec:
      containers:
      - args:
        - sh
        - -ec
        - trap true INT TERM; sleep infinity & wait
        command:
        - sh
        - -ec
        - |
          work_dir="$(pwd)"
          mkdir -p "$XDG_CONFIG_HOME/nats"
          cd "$XDG_CONFIG_HOME/nats"
          if ! [ -s context ]; then
            ln -s /etc/nats-contexts context
          fi
          if ! [ -f context.txt ]; then
            echo -n "default" > context.txt
          fi
          cd "$work_dir"
          exec /entrypoint.sh "$@"
        - --
        image: natsio/nats-box:0.14.3
        name: nats-box
        volumeMounts:
        - mountPath: /etc/nats-contexts
          name: contexts
      enableServiceLinks: false
      volumes:
      - name: contexts
        secret:
          secretName: prism-dev-nats-box-contexts
---
# Source: prism/charts/vault/templates/injector-deployment.yaml
# Deployment for the injector
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prism-dev-vault-agent-injector
  namespace: prism-dev
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    component: webhook
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vault-agent-injector
      app.kubernetes.io/instance: prism-dev
      component: webhook
  
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vault-agent-injector
        app.kubernetes.io/instance: prism-dev
        component: webhook
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vault-agent-injector
                  app.kubernetes.io/instance: "prism-dev"
                  component: webhook
              topologyKey: kubernetes.io/hostname
  
      
      
      
      serviceAccountName: "prism-dev-vault-agent-injector"
      
      securityContext:
        runAsNonRoot: true
        runAsGroup: 1000
        runAsUser: 100
        fsGroup: 1000
      hostNetwork: false
      containers:
        - name: sidecar-injector
          
          image: "hashicorp/vault-k8s:1.4.2"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          env:
            - name: AGENT_INJECT_LISTEN
              value: :8080
            - name: AGENT_INJECT_LOG_LEVEL
              value: info
            - name: AGENT_INJECT_VAULT_ADDR
              value: http://prism-dev-vault.prism-dev.svc:8200
            - name: AGENT_INJECT_VAULT_AUTH_PATH
              value: auth/kubernetes
            - name: AGENT_INJECT_VAULT_IMAGE
              value: "hashicorp/vault:1.17.2"
            - name: AGENT_INJECT_TLS_AUTO
              value: prism-dev-vault-agent-injector-cfg
            - name: AGENT_INJECT_TLS_AUTO_HOSTS
              value: prism-dev-vault-agent-injector-svc,prism-dev-vault-agent-injector-svc.prism-dev,prism-dev-vault-agent-injector-svc.prism-dev.svc
            - name: AGENT_INJECT_LOG_FORMAT
              value: standard
            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN
              value: "false"
            - name: AGENT_INJECT_CPU_REQUEST
              value: "250m"
            - name: AGENT_INJECT_CPU_LIMIT
              value: "500m"
            - name: AGENT_INJECT_MEM_REQUEST
              value: "64Mi"
            - name: AGENT_INJECT_MEM_LIMIT
              value: "128Mi"
            - name: AGENT_INJECT_DEFAULT_TEMPLATE
              value: "map"
            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE
              value: "true"
            
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          args:
            - agent-inject
            - 2>&1
          livenessProbe:
            httpGet:
              path: /health/ready
              port: 8080
              scheme: HTTPS
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
              scheme: HTTPS
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 5
          startupProbe:
            httpGet:
              path: /health/ready
              port: 8080
              scheme: HTTPS
            failureThreshold: 12
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
---
# Source: prism/templates/auth-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prism-dev-auth
spec:
  replicas: 3
  selector:
    matchLabels:
      app: prism-dev
      component: auth
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "8189"
        prometheus.io/scrape: "true"
      labels:
        app: prism-dev
        component: auth
    spec:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      containers:
        - name: prism-dev-auth
          image: "magistrala/auth:latest"
          imagePullPolicy: IfNotPresent
          env:
            - name: MG_JAEGER_URL
              value: "http://magistrala-jaeger-collector:4318/v1/traces"
            - name: MG_JAEGER_TRACE_RATIO
              value: "1"
            - name: MG_SEND_TELEMETRY
              value: "true"
            - name: MG_ES_URL
              value: "magistrala-nats:4222"
            - name: MG_AUTH_LOG_LEVEL
              value: "info"
            - name: MG_AUTH_GRPC_HOST
              value: "0.0.0.0"
            - name: MG_AUTH_GRPC_PORT
              value: "8181"
            - name: MG_AUTH_HTTP_HOST
              value: "0.0.0.0"
            - name: MG_AUTH_HTTP_PORT
              value: "8189"
            - name: MG_AUTH_SECRET
              value: "supersecret"
            - name: MG_AUTH_ACCESS_TOKEN_DURATION
              value: "1h"
            - name: MG_AUTH_REFRESH_TOKEN_DURATION
              value: "24h"
            - name: MG_AUTH_INVITATION_DURATION
              value: "168h"
            - name: MG_AUTH_DB_HOST
              value: prism-dev-postgresqlauth
            - name: MG_AUTH_DB_PORT
              value: "5432"
            - name: MG_AUTH_DB_NAME
              value: "auth"
            - name: MG_AUTH_DB_USER
              value: "magistrala"
            - name: MG_AUTH_DB_PASS
              value: "magistrala"
            - name: MG_SPICEDB_HOST
              value: prism-dev-spicedb-envoy
            - name: MG_SPICEDB_PORT
              value: "50051"
            - name: MG_SPICEDB_SCHEMA_FILE
              value: /schema.zed
            - name: MG_SPICEDB_PRE_SHARED_KEY
              value: "helloworld"
          ports:
              - containerPort: 8189
                protocol: TCP
              - containerPort: 8181
                protocol: TCP
          volumeMounts:
            - mountPath: /schema.zed
              name: spicedb-schema-zed
              subPath: schema.zed
      volumes:
        - name: spicedb-schema-zed
          configMap:
            defaultMode: 256
            name: prism-dev-spicedb-schema-zed
            optional: false
---
# Source: prism/templates/spicedb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prism-dev-spicedb
spec:
  replicas: 3
  selector:
    matchLabels:
      app: prism-dev
      component: spicedb
  template:
    metadata:
      annotations:
        prometheus.io/path: /
        prometheus.io/port: "9090"
        prometheus.io/scrape: "true"
      labels:
        app: prism-dev
        component: spicedb
    spec:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      containers:
        - name: prism-dev-spicedb
          image: "authzed/spicedb:latest"
          imagePullPolicy:  IfNotPresent
          args:
            - serve
            - --grpc-enabled=true
            - --grpc-preshared-key=helloworld
            - --grpc-addr=:50051
            - --log-level=trace
            - --datastore-engine=postgres
            - --datastore-conn-uri=postgres://magistrala:magistrala@prism-dev-postgresqlspicedb:5432/spicedb

            - --dispatch-cluster-enabled=false

            - --http-enabled=false

            - --metrics-enabled=true
            - --metrics-addr=:9090

          ports:
            - containerPort: 50051
              protocol: TCP
              name: grpc
            - containerPort: 9090
              protocol: TCP
              name: metrics
---
# Source: prism/templates/spicedb-envoy-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prism-dev-spicedb-envoy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: prism-dev
      component: spicedb-envoy
  template:
    metadata:
      labels:
        app: prism-dev
        component: spicedb-envoy
    spec:
      containers:
        - name: prism-dev-spicedb-envoy
          image: "envoyproxy/envoy:v1.31-latest"
          imagePullPolicy: 
          ports:
            - containerPort: 50051
              protocol: TCP
          volumeMounts:
            - mountPath: "/etc/envoy/envoy.yaml"
              name: envoy-config
              subPath: envoy.yaml
      volumes:
        - configMap:
            defaultMode: 292  ## equal to 0444 r-- r-- r--
            name: prism-dev-spicedb-envoy-config
            optional: false
          name: envoy-config
---
# Source: prism/templates/users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prism-dev-users
spec:
  selector:
    matchLabels:
      app: prism-dev
      component: users
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9002"
        prometheus.io/scrape: "true"
      labels:
        app: prism-dev
        component: users
    spec:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      containers:
        - name: prism-dev-users
          image: "magistrala/users:latest"
          imagePullPolicy: IfNotPresent
          env:
            - name: MG_JAEGER_URL
              value: "http://magistrala-jaeger-collector:4318/v1/traces"
            - name: MG_JAEGER_TRACE_RATIO
              value: "1"
            - name: MG_SEND_TELEMETRY
              value: "true"
            - name: MG_ES_URL
              value: "magistrala-nats:4222"
            - name: MG_USERS_LOG_LEVEL
              value: "info"
            - name: MG_USERS_HTTP_HOST
              value: "0.0.0.0"
            - name: MG_USERS_HTTP_PORT
              value: "9002"
            - name: MG_TOKEN_RESET_ENDPOINT
              value: "/reset-request"
            - name: MG_USERS_ADMIN_EMAIL
              value: "admin@example.com"
            - name: MG_USERS_ADMIN_PASSWORD
              value: "12345678"
            - name: MG_USERS_SECRET_KEY
              value: "secretKey"
            - name: MG_USERS_PASS_REGEX
              value: "^.{8,}$"
            - name: MG_USERS_ALLOW_SELF_REGISTER
              value: "true"
            - name: MG_USERS_DELETE_INTERVAL
              value: "24h"
            - name: MG_USERS_DELETE_AFTER
              value: "720h"
            - name: MG_USERS_DB_HOST
              value: "prism-dev-postgresqlusers"
            - name: MG_USERS_DB_PORT
              value: "5432"
            - name: MG_USERS_DB_NAME
              value: "users"
            - name: MG_USERS_DB_USER
              value: "magistrala"
            - name: MG_USERS_DB_PASS
              value: "magistrala"
            - name : MG_AUTH_GRPC_URL
              value: prism-dev-envoy:8181
          ports:
            - containerPort: 9002
              protocol: TCP
          volumeMounts:
            - mountPath: /email.tmpl
              name: users-config
              subPath: email.tmpl
      volumes:
        - name: users-config
          configMap:
            defaultMode: 256
            name: prism-dev-users-config
            optional: false
---
# Source: prism/templates/hpa.yaml
# Copyright (c) Ultraviolet
# SPDX-License-Identifier: Apache-2.0

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: prism-dev-users
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: prism-dev-users
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
---
# Source: prism/charts/jaeger/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.15.3
    release: prism-dev
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: cassandra
      release: prism-dev
  serviceName: prism-dev-cassandra
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  template:
    metadata:
      labels:
        app: cassandra
        release: prism-dev
    spec:
      hostNetwork: false
      containers:
      - name: prism-dev-cassandra
        image: "cassandra:3.11.6"
        imagePullPolicy: "IfNotPresent"
        resources:
          {}
        env:
        - name: CASSANDRA_SEEDS
          value: "prism-dev-cassandra-0.prism-dev-cassandra.prism-dev.svc.cluster.local"
        - name: MAX_HEAP_SIZE
          value: "2048M"
        - name: HEAP_NEWSIZE
          value: "512M"
        - name: CASSANDRA_ENDPOINT_SNITCH
          value: "GossipingPropertyFileSnitch"
        - name: CASSANDRA_CLUSTER_NAME
          value: "jaeger"
        - name: CASSANDRA_DC
          value: "dc1"
        - name: CASSANDRA_RACK
          value: "rack1"
        - name: CASSANDRA_START_RPC
          value: "false"
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status | grep -E \"^UN\\s+${POD_IP}\"" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        ports:
        - name: intra
          containerPort: 7000
        - name: tls
          containerPort: 7001
        - name: jmx
          containerPort: 7199
        - name: cql
          containerPort: 9042
        - name: thrift
          containerPort: 9160
        volumeMounts:
        - name: data
          mountPath: /var/lib/cassandra
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "exec nodetool decommission"]
      terminationGracePeriodSeconds: 30
      volumes:
      - name: data
        emptyDir: {}
---
# Source: prism/charts/nats/templates/stateful-set.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: nats
  serviceName: prism-dev-nats-headless
  template:
    metadata:
      annotations:
        checksum/config: 4dd0af02430f814622e9c5f5cc5bbc83cb876dc328fb8f5ce7e4df5410a81c01
      labels:
        app.kubernetes.io/component: nats
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nats
        app.kubernetes.io/version: 2.10.17
        helm.sh/chart: nats-1.2.1
    spec:
      containers:
      - args:
        - --config
        - /etc/nats-config/nats.conf
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        image: nats:2.10.17-alpine
        lifecycle:
          preStop:
            exec:
              command:
              - nats-server
              - -sl=ldm=/var/run/nats/nats.pid
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-enabled-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: nats
        ports:
        - containerPort: 4222
          name: nats
        - containerPort: 8222
          name: monitor
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-server-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        startupProbe:
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        volumeMounts:
        - mountPath: /etc/nats-config
          name: config
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /data
          name: prism-dev-nats-js
      - args:
        - -pid
        - /var/run/nats/nats.pid
        - -config
        - /etc/nats-config/nats.conf
        image: natsio/nats-server-config-reloader:0.15.0
        name: reloader
        volumeMounts:
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /etc/nats-config
          name: config
      enableServiceLinks: false
      shareProcessNamespace: true
      volumes:
      - configMap:
          name: prism-dev-nats-config
        name: config
      - emptyDir: {}
        name: pid
  volumeClaimTemplates:
  - metadata:
      name: prism-dev-nats-js
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
# Source: prism/charts/postgresqlauth/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-postgresqlauth
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlauth
    helm.sh/chart: postgresqlauth-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: prism-dev-postgresqlauth-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresqlauth
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: prism-dev-postgresqlauth
      labels:
        app.kubernetes.io/name: postgresqlauth
        helm.sh/chart: postgresqlauth-12.5.6
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "15.3.0"
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: prism-dev
                    app.kubernetes.io/name: postgresqlauth
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.3.0-debian-11-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "magistrala"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlauth
                  key: postgres-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlauth
                  key: password
            - name: POSTGRES_DB
              value: "auth"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "magistrala" -d "dbname=auth" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "magistrala" -d "dbname=auth" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: prism/charts/postgresqlspicedb/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-postgresqlspicedb
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlspicedb
    helm.sh/chart: postgresqlspicedb-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: prism-dev-postgresqlspicedb-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresqlspicedb
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: prism-dev-postgresqlspicedb
      labels:
        app.kubernetes.io/name: postgresqlspicedb
        helm.sh/chart: postgresqlspicedb-12.5.6
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "15.3.0"
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: prism-dev
                    app.kubernetes.io/name: postgresqlspicedb
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.3.0-debian-11-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "magistrala"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlspicedb
                  key: postgres-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlspicedb
                  key: password
            - name: POSTGRES_DB
              value: "spicedb"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "magistrala" -d "dbname=spicedb" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "magistrala" -d "dbname=spicedb" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: prism/charts/postgresqlusers/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-postgresqlusers
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/name: postgresqlusers
    helm.sh/chart: postgresqlusers-12.5.6
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: prism-dev-postgresqlusers-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresqlusers
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: prism-dev-postgresqlusers
      labels:
        app.kubernetes.io/name: postgresqlusers
        helm.sh/chart: postgresqlusers-12.5.6
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "15.3.0"
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: prism-dev
                    app.kubernetes.io/name: postgresqlusers
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.3.0-debian-11-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "magistrala"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlusers
                  key: postgres-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-postgresqlusers
                  key: password
            - name: POSTGRES_DB
              value: "users"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "magistrala" -d "dbname=users" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "magistrala" -d "dbname=users" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: prism/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-redis-master
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: prism-dev-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.5
        helm.sh/chart: redis-19.6.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 6f88317aecc53f594f5603415ce2975f15b0c7d646ae34ef7dc5d85670e10783
        checksum/secret: a0602f04b7de7ac68aaa4eba009a4b348179cd604cc4e479ce8b08c79fde233f
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: prism-dev-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: prism-dev
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.5-debian-12-r2
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: prism-dev-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: prism-dev-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: prism-dev-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: prism-dev
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: prism/charts/redis/templates/replicas/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-redis-replicas
  namespace: "prism-dev"
  labels:
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.6.2
    app.kubernetes.io/component: replica
spec:
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: prism-dev
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: replica
  serviceName: prism-dev-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: prism-dev
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.5
        helm.sh/chart: redis-19.6.2
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 6f88317aecc53f594f5603415ce2975f15b0c7d646ae34ef7dc5d85670e10783
        checksum/secret: 34d901e1fd6c42743c2ef5fec5b35bfbb46ce1465dca2dbcfeade8b1494370df
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: prism-dev-redis-replica
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: prism-dev
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.5-debian-12-r2
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: prism-dev-redis-master-0.prism-dev-redis-headless.prism-dev.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-redis
                  key: redis-password
            - name: REDIS_MASTER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: prism-dev-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: prism-dev-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: prism-dev-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: prism-dev-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: prism-dev
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: prism/charts/vault/templates/server-statefulset.yaml
# StatefulSet to run the actual vault server cluster.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prism-dev-vault
  namespace: prism-dev
  labels:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: prism-dev-vault-internal
  podManagementPolicy: Parallel
  replicas: 1
  updateStrategy:
    type: OnDelete
  selector:
    matchLabels:
      app.kubernetes.io/name: vault
      app.kubernetes.io/instance: prism-dev
      component: server
  template:
    metadata:
      labels:
        helm.sh/chart: vault-0.28.1
        app.kubernetes.io/name: vault
        app.kubernetes.io/instance: prism-dev
        component: server
      annotations:
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vault
                  app.kubernetes.io/instance: "prism-dev"
                  component: server
              topologyKey: kubernetes.io/hostname
  
      
      
      
      terminationGracePeriodSeconds: 10
      serviceAccountName: prism-dev-vault
      
      securityContext:
        runAsNonRoot: true
        runAsGroup: 1000
        runAsUser: 100
        fsGroup: 1000
      hostNetwork: false
      volumes:
        
        - name: config
          configMap:
            name: prism-dev-vault-config
  
        - name: home
          emptyDir: {}
      containers:
        - name: vault
          
          image: hashicorp/vault:1.17.2
          imagePullPolicy: IfNotPresent
          command:
          - "/bin/sh"
          - "-ec"
          args: 
          - |
            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;
            [ -n "${HOST_IP}" ] && sed -Ei "s|HOST_IP|${HOST_IP?}|g" /tmp/storageconfig.hcl;
            [ -n "${POD_IP}" ] && sed -Ei "s|POD_IP|${POD_IP?}|g" /tmp/storageconfig.hcl;
            [ -n "${HOSTNAME}" ] && sed -Ei "s|HOSTNAME|${HOSTNAME?}|g" /tmp/storageconfig.hcl;
            [ -n "${API_ADDR}" ] && sed -Ei "s|API_ADDR|${API_ADDR?}|g" /tmp/storageconfig.hcl;
            [ -n "${TRANSIT_ADDR}" ] && sed -Ei "s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g" /tmp/storageconfig.hcl;
            [ -n "${RAFT_ADDR}" ] && sed -Ei "s|RAFT_ADDR|${RAFT_ADDR?}|g" /tmp/storageconfig.hcl;
            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl 
   
          securityContext:
            allowPrivilegeEscalation: false
          env:
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VAULT_K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VAULT_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: VAULT_ADDR
              value: "http://127.0.0.1:8200"
            - name: VAULT_API_ADDR
              value: "http://$(POD_IP):8200"
            - name: SKIP_CHOWN
              value: "true"
            - name: SKIP_SETCAP
              value: "true"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VAULT_CLUSTER_ADDR
              value: "https://$(HOSTNAME).prism-dev-vault-internal:8201"
            - name: HOME
              value: "/home/vault"
            
            
            
          volumeMounts:
          
  
    
            - name: data
              mountPath: /vault/data
    
  
  
            - name: config
              mountPath: /vault/config
  
            - name: home
              mountPath: /home/vault
          ports:
            - containerPort: 8200
              name: http
            - containerPort: 8201
              name: https-internal
            - containerPort: 8202
              name: http-rep
          readinessProbe:
            # Check status; unsealed vault servers return 0
            # The exit code reflects the seal status:
            #   0 - unsealed
            #   1 - error
            #   2 - sealed
            exec:
              command: ["/bin/sh", "-ec", "vault status -tls-skip-verify"]
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          lifecycle:
            # Vault container doesn't receive SIGTERM from Kubernetes
            # and after the grace period ends, Kube sends SIGKILL.  This
            # causes issues with graceful shutdowns such as deregistering itself
            # from Consul (zombie services).
            preStop:
              exec:
                command: [
                  "/bin/sh", "-c",
                  # Adding a sleep here to give the pod eviction a
                  # chance to propagate, so requests will not be made
                  # to this pod while it's terminating
                  "sleep 5 && kill -SIGTERM $(pidof vault)",
                ]
      
  
  volumeClaimTemplates:
    - metadata:
        name: data
      
      
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
---
# Source: prism/templates/spicedb-migration-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: prism-dev-spicedb-migrate-job
  labels:
    app: prism-dev
    component: spicedb-migrate-job
spec:
  template:
    spec:
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      containers:
        - name: prism-dev-spicedb-migrate
          image: "authzed/spicedb:latest"
          imagePullPolicy:  IfNotPresent
          command: ["spicedb"]
          args:
            - migrate
            - head
            - --datastore-engine=postgres
            - --datastore-conn-uri=postgres://magistrala:magistrala@prism-dev-postgresqlspicedb:5432/spicedb
---
# Source: prism/charts/vault/templates/injector-mutating-webhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: prism-dev-vault-agent-injector-cfg
  labels:
    app.kubernetes.io/name: vault-agent-injector
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
webhooks:
  - name: vault.hashicorp.com
    failurePolicy: Ignore
    matchPolicy: Exact
    sideEffects: None
    timeoutSeconds: 30
    admissionReviewVersions: ["v1", "v1beta1"]
    clientConfig:
      service:
        name: prism-dev-vault-agent-injector-svc
        namespace: prism-dev
        path: "/mutate"
      caBundle: ""
    rules:
      - operations: ["CREATE"]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
        scope: "Namespaced"
    objectSelector:
      matchExpressions:
      - key: app.kubernetes.io/name
        operator: NotIn
        values:
        - vault-agent-injector
---
# Source: prism/charts/nats/templates/tests/request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: test-request-reply
    app.kubernetes.io/instance: prism-dev
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.17
    helm.sh/chart: nats-1.2.1
  name: prism-dev-nats-test-request-reply
spec:
  containers:
  - args:
    - sh
    - -ec
    - nats reply --echo echo & pid="$!"; sleep 1; nats request echo hi > /tmp/resp;
      kill "$pid"; wait; grep -qF hi /tmp/resp
    command:
    - sh
    - -ec
    - |
      work_dir="$(pwd)"
      mkdir -p "$XDG_CONFIG_HOME/nats"
      cd "$XDG_CONFIG_HOME/nats"
      if ! [ -s context ]; then
        ln -s /etc/nats-contexts context
      fi
      if ! [ -f context.txt ]; then
        echo -n "default" > context.txt
      fi
      cd "$work_dir"
      exec /entrypoint.sh "$@"
    - --
    image: natsio/nats-box:0.14.3
    name: nats-box
    volumeMounts:
    - mountPath: /etc/nats-contexts
      name: contexts
  enableServiceLinks: false
  restartPolicy: Never
  volumes:
  - name: contexts
    secret:
      secretName: prism-dev-nats-box-contexts
---
# Source: prism/charts/vault/templates/tests/server-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: prism-dev-vault-server-test
  namespace: prism-dev
  annotations:
    "helm.sh/hook": test
spec:
  
  containers:
    - name: prism-dev-server-test
      image: hashicorp/vault:1.17.2
      imagePullPolicy: IfNotPresent
      env:
        - name: VAULT_ADDR
          value: http://prism-dev-vault.prism-dev.svc:8200
        
      command:
        - /bin/sh
        - -c
        - |
          echo "Checking for sealed info in 'vault status' output"
          ATTEMPTS=10
          n=0
          until [ "$n" -ge $ATTEMPTS ]
          do
            echo "Attempt" $n...
            vault status -format yaml | grep -E '^sealed: (true|false)' && break
            n=$((n+1))
            sleep 5
          done
          if [ $n -ge $ATTEMPTS ]; then
            echo "timed out looking for sealed info in 'vault status' output"
            exit 1
          fi

          exit 0
      volumeMounts:
  volumes:
  restartPolicy: Never
